{
    "contents" : "---\ntitle: \"Assignment 3 - Earnings Calls Analysis\"\nauthor: \"Dave Hurst\"\ndate: \"Sunday, November 02, 2014\"\noutput: pdf_document\n---\nThe goal of this exercise is to explore the corpus of Earnings Calls transcripts using R.  The corpus is made up of transcripts for the primary two hard disk drivers, Seagate (STX) and Western Digital (WDC).  The transcripts for each quarter were divided into two separate documents, one for the corporate report, and one for Q&A.  These documents were gathered for the fical Q4 (ending in June) 2013, Q4 2014, and Q1 2015, totalling 12 documents in all.\n\nLoad R packages:\n```{r results='hide', message=FALSE, warning=FALSE}\nlibrary(tm) # Framework for text mining.\nlibrary(SnowballC) # Provides wordStem() for stemming.\nlibrary(RColorBrewer) # Generate palette of colours for plots.\nlibrary(ggplot2) # Plot word frequencies.\nlibrary(magrittr)\n#library(Rgraphviz) # Correlation plots  #no package available for 3.1.1 from cran\n```\nCorpus (text documents) are in a folder in the working directory named EarningsCalls\n```{r echo=FALSE, results='hide'}\nsetwd(\"C:/Users/Dave/Google Drive/Predictive Analytics/453 - Text Analytics/R453 Text Analytics\")\n```\n```{r}\ncorpus_dir <- \"EarningsCalls\"\ncname <- file.path(\".\", corpus_dir)\nnfiles <- length(dir(cname))\n```\nThere are `r nfiles` files in corpus directory (`r corpus_dir`)\nThe text files are available for download here: http://goo.gl/UTF1Fg\n```{r}\ndir(corpus_dir)\ndocs <- Corpus(DirSource(cname))\nclass(docs)\nclass(docs[[1]])\n#inspect(docs[1])  #commented out since this output is large\n\ntoSpace <- content_transformer(function(x, pattern) gsub(pattern, \" \", x))\n\n# doc style specific transfomration\ndocs <- tm_map(docs, toSpace, \"http:\\\\S*\") #remove URLS\ndocs <- tm_map(docs, toSpace, \"<.*/.*>\") #remove page numbers\ndocs <- tm_map(docs, toSpace, \"\\\\n\") #remove newline chars\n#TO-DO: remove unprintable characters\n\ngetTransformations() # this is the list of things we can do to the corpus\n\ndocs <- tm_map(docs, content_transformer(tolower))\ndocs <- tm_map(docs, content_transformer(removePunctuation))\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\ndocs <- tm_map(docs, removeWords, c(\"question\")) #equivalent to SPSS concept delete\ndocs <- tm_map(docs, stripWhitespace)\ndocs <- tm_map(docs, removeNumbers)  #may want to reconsider doing this\n\n#docs <- tm_map(docs, stemDocument)  #step words -- not sure we want to do this\n#TO-DO: compare the differences of stemming and non stemming\n\n```\nNow the the data is prepared we can create a document term matrix and explore the frequencies\n\n```{r}\n\ndtm <- DocumentTermMatrix(docs)\ndtm\nnterms <- dtm$ncol\ninspect(dtm[1:5, c(12:14, 1000:1002, seq(nterms-2,nterms))])  #first 11 have unprintable chars\n\nfreq <- colSums(as.matrix(dtm))\nord <- order(freq)\nfreq.most <- freq[tail(ord)]\nprint (\"The most frequent terms found are:\")\nfreq.most\n\nterms <- names(freq)\nprint (\"Find terms with string 'product'\")\nfreq[grep(\"product\",terms)]\n\nbarplot(freq[grep(\"^cloud$|^hybrid|^product$|products\",terms)])  #word boundaries need work\n#\n#TO-DOplay around with synonyms\n#synonyms \"line product,product,product line,product lines,product offering,product offerings\"\n```\n\n\n",
    "created" : 1415415616559.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1173855710",
    "id" : "AD98DABF",
    "lastKnownWriteTime" : 1415427064,
    "path" : "C:/Users/Dave/Google Drive/Predictive Analytics/453 - Text Analytics/R453 Text Analytics/Assignment 3- R analysis.Rmd",
    "project_path" : "Assignment 3- R analysis.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}