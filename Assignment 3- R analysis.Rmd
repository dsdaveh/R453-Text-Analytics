---
title: "Assignment 3 - Earnings Calls Analysis"
author: "Dave Hurst"
date: "Sunday, November 02, 2014"
output: pdf_document
---
The goal of this exercise is to explore the corpus of Earnings Calls transcripts using R.  The corpus is made up of transcripts for the primary two hard disk drivers, Seagate (STX) and Western Digital (WDC).  The transcripts for each quarter were divided into two separate documents, one for the corporate report, and one for Q&A.  These documents were gathered for the fical Q4 (ending in June) 2013, Q4 2014, and Q1 2015, totalling 12 documents in all.

Load R packages:
```{r results='hide', message=FALSE, warning=FALSE}
library(tm) # Framework for text mining.
library(SnowballC) # Provides wordStem() for stemming.
library(RColorBrewer) # Generate palette of colours for plots.
library(ggplot2) # Plot word frequencies.
library(magrittr)
#library(Rgraphviz) # Correlation plots  #no package available for 3.1.1 from cran
```
Corpus (text documents) are in a folder in the working directory named EarningsCalls
```{r echo=FALSE, results='hide'}
setwd("C:/Users/Dave/Google Drive/Predictive Analytics/453 - Text Analytics/R453 Text Analytics")
```
```{r}
corpus_dir <- "EarningsCalls"
cname <- file.path(".", corpus_dir)
nfiles <- length(dir(cname))
```
There are `r nfiles` files in corpus directory (`r corpus_dir`)
The text files are available for download here: http://goo.gl/UTF1Fg
```{r}
dir(corpus_dir)
docs <- Corpus(DirSource(cname))
class(docs)
class(docs[[1]])
#inspect(docs[1])  #commented out since this output is large

toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))

# doc style specific transfomration
docs <- tm_map(docs, toSpace, "http:\\S*") #remove URLS
docs <- tm_map(docs, toSpace, "<.*/.*>") #remove page numbers
docs <- tm_map(docs, toSpace, "\\n") #remove newline chars
#TO-DO: remove unprintable characters

getTransformations() # this is the list of things we can do to the corpus

docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(removePunctuation))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("question")) #equivalent to SPSS concept delete
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removeNumbers)  #may want to reconsider doing this

#docs <- tm_map(docs, stemDocument)  #step words -- not sure we want to do this
#TO-DO: compare the differences of stemming and non stemming

```
Now the the data is prepared we can create a document term matrix and explore the frequencies

```{r}

dtm <- DocumentTermMatrix(docs)
dtm
nterms <- dtm$ncol
inspect(dtm[1:5, c(12:14, 1000:1002, seq(nterms-2,nterms))])  #first 11 have unprintable chars

freq <- colSums(as.matrix(dtm))
ord <- order(freq)
freq.most <- freq[tail(ord)]
print ("The most frequent terms found are:")
freq.most

terms <- names(freq)
print ("Find terms with string 'product'")
freq[grep("product",terms)]

barplot(freq[grep("^cloud$|^hybrid|^product$|products",terms)])  #word boundaries need work
#
#TO-DOplay around with synonyms
#synonyms "line product,product,product line,product lines,product offering,product offerings"
```


